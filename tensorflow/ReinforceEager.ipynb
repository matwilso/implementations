{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, input_shape=(8,))\n",
    "        self.dense2 = tf.keras.layers.Dense(units=4)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = tf.nn.softmax(result)\n",
    "        return result\n",
    "\n",
    "pi = PolicyNetwork()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854f28>', '<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854da0>', '<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854d30>', '<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854d68>'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-70c411bcdc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-70c411bcdc9e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mrunning_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mfinish_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
      "\u001b[0;32m<ipython-input-12-70c411bcdc9e>\u001b[0m in \u001b[0;36mfinish_episode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     optimizer.apply_gradients(zip(grads, pi.trainable_variables),\n\u001b[0;32m---> 29\u001b[0;31m                             global_step=tf.train.get_or_create_global_step())\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m--> 598\u001b[0;31m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854f28>', '<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854da0>', '<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854d30>', '<tensorflow.python.training.optimizer._DenseResourceVariableProcessor object at 0x7f41d5854d68>']."
     ]
    }
   ],
   "source": [
    "def select_action(state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = tf.expand_dims(tf.constant(state, dtype=tf.float32), 0)\n",
    "        probs = pi(state)\n",
    "        m = tf.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        pi.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.numpy()[0]\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    with tf.GradientTape() as tape:\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        rewards = []\n",
    "        for r in pi.rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            rewards.insert(0, R)\n",
    "        rewards = tf.constant(rewards)\n",
    "        rew_mean, rew_var = tf.nn.moments(rewards, axes=[0])\n",
    "        rewards = (rewards - rew_mean) / (rew_var + eps)\n",
    "        for log_prob, reward in zip(pi.saved_log_probs, rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "        policy_loss = tf.reduce_sum(policy_loss, 0)\n",
    "    \n",
    "    grads = tape.gradient(policy_loss, pi.trainable_variables)\n",
    "    print(grads)\n",
    "    optimizer.apply_gradients(zip(grads, pi.trainable_variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    del pi.rewards[:]\n",
    "    del pi.saved_log_probs[:]\n",
    "\n",
    "\n",
    "render = True\n",
    "log_interval = 100\n",
    "gamma = 0.99\n",
    "eps = 1e-9\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            pi.rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = running_reward * 0.99 + t * 0.01\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "                i_episode, t, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
